## Explainable Computer Vision with embeddings and KNN classifier.

*Authors: Olaf Werner, Bogdan JastrzÄ™bski (Warsaw University of Technology)*

### Abstract


### Introduction
Computer vision is widely known use case for neural networks. However neural networks are infamous for their complexity and lack of interpretability. On the other hand simple classifiers like KNN have really poor results for complex tasks like image recognition. In this article we will prove that it is possible to get best of both worlds using emmbeddings. 

### Data
We are going to use dataset [Fashion-Mnist](https://www.openml.org/d/40996). Fashion-MNIST is a dataset of Zalando's article images, consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Classes are following: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot.

### Methodology

The simplest and one of the most robust classifiers is KNN. It doesn't generalize information, instead it saves training dataset and during prediction it finds the most similar historical observations and predicts label of a new observation based on their labels.

However, it not only doesn't have the capacity to distinguish important features from not important ones, but also to find more complex interactions between variables. 

One way to improve KNN's performance is to preced closest neighbour computation with transformation of the space of observations, so that the derivative variables are more meaningful. Such beneficial transformation is called embedding. It can be done in various ways. 

The question is, whether or not the new classifier is interpretable. We argue, that it is. The main reason is that even if we can't interpret the embedding part, we can at least provide historical data that our model used to make prediction. Someone could argue, that it can be done with every classifier, just by finding training data that obtains the most similar prediction. However, with our classifier we can say for sure, that prediction were purely made based on the most similar cases in our dataset, which is fundamentally not true about different classifiers.

An embedding can be done made in various ways. In this article we will explore different embedding techniques, including:

* SVD embedding

* Logistic Regression Autoencoder

* Self Organizing Maps

* Convolutional Autoencoder

### Logistic Regression
Logistic regression is basic classification model. We assume


### K-means
Our problem is supervised one, but we can still use unsupervised aproach to get better results. We use K-means algorithm (also known as Lloyd's algorithm) to find subclasses in every class. 

#### Algorithm 
1. Initiate number of random centroids

2. For every observation find nearest centroid

3. Calculate average of observations in every group found in point 2

4. This averages becomes new centroids

5. Repeat points 2 to 4 until all new centroids are at the distance less then $\epsilon$ from old centroids

We use euclidean distance.

Prediction for every new observation is simply class of nearest centroid. Algorithm is interpretable, because we can visualise centroids as images. Thanks to using K-means to find subclasses our images are not blurry.
Also because number of all subclasses is much lower than number of records in data set using KNN only on centroids is much faster.


### SVD
SVD is standard method of method of dimensionality reduction. It is rewriting $m\times n$ matrix $M$ as $U\Sigma V^T$ where $U$ is $m\times m$ orthonormal matrix, $V^T$ is $n\times n$ orthonormal matrix and $\Sigma$ is $m\times n$ rectangular diagonal matrix with non-negative real numbers on the diagonal. We assume that singular values of $\Sigma$ are in descending order. Now by taking first columns of $V^T$ we get vectors which are the most relevent.

We can then visualise this vectors and see which parts of the picture are the most importent. Also we can reduce number of dimensions for KNN.

### Results


### Conclusions
